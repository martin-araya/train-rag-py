# API RAG con Robyn, ChromaDB y LangChain

Una API de alto rendimiento de GeneraciÃ³n Aumentada por RecuperaciÃ³n (RAG) construida con **Robyn**, **ChromaDB** y **LangChain** para procesamiento de documentos y consultas inteligentes. Este sistema te permite subir documentos PDF, procesarlos en una base de datos vectorial y realizar consultas en lenguaje natural contra el contenido.

## ğŸ—ï¸ Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Subida de PDF  â”‚â”€â”€â”€â–¶â”‚ Divisor de Texto â”‚â”€â”€â”€â–¶â”‚    ChromaDB     â”‚
â”‚                 â”‚    â”‚    (Chunks)      â”‚    â”‚ (Vector Store)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚ Consulta Usuarioâ”‚â”€â”€â”€â–¶â”‚  Pipeline RAG    â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                 â”‚    â”‚   (LangChain)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Ollama LLM     â”‚
                       â”‚   Respuesta      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ CaracterÃ­sticas

- **Procesamiento RÃ¡pido de PDF**: Sube y procesa documentos PDF con PyMuPDF
- **BÃºsqueda Vectorial**: BÃºsqueda semÃ¡ntica usando ChromaDB y embeddings de Ollama
- **Respuestas Inteligentes**: Respuestas contextualmente conscientes usando LLM de Ollama
- **Alto Rendimiento**: Construido con Robyn para rendimiento asÃ­ncrono
- **Soporte CORS**: Listo para integraciÃ³n con frontend
- **Monitoreo de Salud**: Verificaciones de salud y endpoints de debug integrados
- **Preguntas RÃ¡pidas**: Consultas pre-optimizadas para respuestas mÃ¡s veloces

## ğŸ“‹ Requisitos Previos

Antes de ejecutar este proyecto, asegÃºrate de tener los siguientes servicios ejecutÃ¡ndose:

### 1. Servidor ChromaDB
```bash
# Usando Docker (recomendado)
docker run -p 8000:8000 chromadb/chroma

# O instalar localmente
pip install chromadb
chroma run --host 0.0.0.0 --port 8000
```

### 2. Servidor Ollama
```bash
# Instalar Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Descargar los modelos necesarios
ollama pull granite-embedding:278m  # Para embeddings
ollama pull llama3.2:3b-instruct-q6_K  # Para generaciÃ³n de texto

# Iniciar el servidor Ollama (normalmente corre en el puerto 11434)
ollama serve
```

## ğŸ› ï¸ InstalaciÃ³n

1. **Clonar el repositorio**
```bash
git clone <url-del-repositorio>
cd rag-api
```

2. **Crear entorno virtual**
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

3. **Instalar dependencias**
```bash
pip install -r requirements.txt
```

### Requirements.txt
```txt
robyn
chromadb-client
langchain
langchain-community
pymupdf
sentence-transformers
langchain_core
```

## âš™ï¸ ConfiguraciÃ³n

La API usa la siguiente configuraciÃ³n por defecto:

```python
# ConfiguraciÃ³n ChromaDB
CHROMA_HOST = "localhost"
CHROMA_PORT = 8000
COLLECTION_NAME = "pdf_documents_collection"

# ConfiguraciÃ³n Ollama
OLLAMA_BASE_URL = "http://localhost:11434"
OLLAMA_EMBEDDING_MODEL = "granite-embedding:278m"
OLLAMA_GENERATION_MODEL = "llama3.2:3b-instruct-q6_K"

# ConfiguraciÃ³n del Servidor
HOST = "0.0.0.0"
PORT = 8080
```

Para modificar estas configuraciones, edita las constantes al principio del archivo Python principal.

## ğŸƒâ€â™‚ï¸ Ejecutar la AplicaciÃ³n

1. **Iniciar los servicios requeridos** (ChromaDB y Ollama)

2. **Ejecutar el servidor de la API**
```bash
python main.py
```

El servidor iniciarÃ¡ en `http://localhost:8080`

## ğŸ“š Endpoints de la API

### Endpoints Principales

#### `POST /upload-pdf`
Subir y procesar un documento PDF.

**PeticiÃ³n:**
```bash
curl -X POST \
  -F "upload_file=@documento.pdf" \
  http://localhost:8080/upload-pdf
```

**Respuesta:**
```json
{
  "message": "Archivo 'documento.pdf' procesado exitosamente.",
  "chunks_added": 45,
  "status": "success"
}
```

#### `GET /query?q={pregunta}`
Consultar los documentos procesados.

**PeticiÃ³n:**
```bash
curl "http://localhost:8080/query?q=Â¿CuÃ¡l%20es%20el%20tema%20principal%20de%20este%20documento?"
```

**Respuesta:**
```json
{
  "query": "Â¿CuÃ¡l es el tema principal de este documento?",
  "answer": "El documento discute tÃ©cnicas de aprendizaje automÃ¡tico...",
  "status": "success",
  "docs_found": 3,
  "processing_time": "1.23s",
  "context_length": 1850
}
```

### Endpoints de InformaciÃ³n

#### `GET /doc-info`
Obtener informaciÃ³n bÃ¡sica sobre los documentos cargados.

**Respuesta:**
```json
{
  "total_chunks": 45,
  "sample_content": "Este paper de investigaciÃ³n presenta...",
  "metadata": {
    "filename": "investigacion.pdf"
  },
  "content_preview": [
    {
      "chunk": 1,
      "preview": "Contenido de la secciÃ³n de introducciÃ³n..."
    }
  ]
}
```

#### `GET /quick-questions`
Obtener preguntas sugeridas y consejos para consultar documentos.

**Respuesta:**
```json
{
  "suggested_questions": [
    "Â¿De quÃ© trata este documento?",
    "Â¿CuÃ¡l es el tema principal?",
    "Â¿QuÃ© mÃ©todos se describen?",
    "Â¿CuÃ¡les son las conclusiones principales?"
  ],
  "document_type": "Research Paper/Academic Document",
  "content_sample": "...",
  "tips": [
    "Pregunta sobre 'mÃ©todos' o 'methodology'",
    "Pregunta sobre 'resultados' o 'results'"
  ]
}
```

### Endpoints del Sistema

#### `GET /health`
Verificar el estado de salud de todos los servicios.

**Respuesta:**
```json
{
  "status": "healthy",
  "chromadb": true,
  "ollama": true,
  "timestamp": "2024-01-15T10:30:00"
}
```

#### `GET /debug`
Obtener informaciÃ³n detallada de depuraciÃ³n.

**Respuesta:**
```json
{
  "vector_store": true,
  "llm": true,
  "chroma_client": true,
  "collection_name": "pdf_documents_collection",
  "ollama_base_url": "http://localhost:11434"
}
```

#### `GET /`
Obtener informaciÃ³n de la API y endpoints disponibles.

## ğŸ”§ IntegraciÃ³n con Frontend

La API incluye soporte CORS para aplicaciones frontend. Ejemplo de uso con JavaScript:

```javascript
// Subir PDF
const subirPDF = async (archivo) => {
  const formData = new FormData();
  formData.append('upload_file', archivo);
  
  const respuesta = await fetch('http://localhost:8080/upload-pdf', {
    method: 'POST',
    body: formData
  });
  
  return await respuesta.json();
};

// Consultar documentos
const consultarDocumentos = async (pregunta) => {
  const respuesta = await fetch(
    `http://localhost:8080/query?q=${encodeURIComponent(pregunta)}`
  );
  
  return await respuesta.json();
};
```

## ğŸ› SoluciÃ³n de Problemas

### Problemas Comunes

1. **Error de ConexiÃ³n con ChromaDB**
   ```
   âŒ No se pudo conectar a ChromaDB
   ```
   - AsegÃºrate de que ChromaDB estÃ© ejecutÃ¡ndose en el puerto 8000
   - Verifica si el puerto estÃ¡ disponible: `netstat -an | grep 8000`

2. **Modelo de Ollama No Encontrado**
   ```
   âŒ Error configurando modelo de generaciÃ³n
   ```
   - Descarga los modelos requeridos: `ollama pull granite-embedding:278m`
   - Verifica que los modelos estÃ©n disponibles: `ollama list`

3. **Error de Procesamiento de PDF**
   ```
   âŒ No se pudo procesar el contenido del PDF
   ```
   - AsegÃºrate de que el archivo subido sea un PDF vÃ¡lido
   - Verifica permisos de archivo y lÃ­mites de tamaÃ±o

4. **Respuestas Lentas en Consultas**
   - Considera usar modelos mÃ¡s pequeÃ±os para respuestas mÃ¡s rÃ¡pidas
   - Reduce el tamaÃ±o de chunk en la configuraciÃ³n
   - Verifica recursos del sistema (CPU, memoria)

### OptimizaciÃ³n de Rendimiento

1. **Reducir TamaÃ±o del Modelo**: Usa modelos de Ollama mÃ¡s pequeÃ±os para respuestas mÃ¡s rÃ¡pidas
2. **Optimizar TamaÃ±o de Chunk**: Ajusta el parÃ¡metro `chunk_size` (predeterminado: 1000)
3. **Limitar Contexto**: Reduce `max_context_length` para procesamiento LLM mÃ¡s rÃ¡pido
4. **Usar GPU**: Configura Ollama para usar aceleraciÃ³n GPU si estÃ¡ disponible

## ğŸ“Š Monitoreo

La API proporciona varias capacidades de monitoreo:

- **Verificaciones de Salud**: Endpoint `/health` para estado de servicios
- **Info de Debug**: Endpoint `/debug` para detalles de configuraciÃ³n
- **MÃ©tricas de Procesamiento**: Tiempos de respuesta de consultas y conteos de documentos
- **Logging**: Logging detallado en consola con emojis para fÃ¡cil lectura

## ğŸ”’ Consideraciones de Seguridad

- La API actualmente permite todos los orÃ­genes CORS para desarrollo
- La validaciÃ³n de subida de archivos es bÃ¡sica (solo extensiÃ³n de archivo)
- Considera agregar autenticaciÃ³n para uso en producciÃ³n
- Implementa limitaciÃ³n de velocidad para despliegues pÃºblicos

## ğŸš€ Despliegue en ProducciÃ³n

Para despliegue en producciÃ³n:

1. **Usar Variables de Entorno** para configuraciÃ³n
2. **Configurar Proxy Inverso** (nginx/Apache)
3. **Configurar SSL/TLS**
4. **Configurar Logging Apropiado**
5. **Implementar Monitoreo de Salud**
6. **Usar Gestor de Procesos** (PM2, systemd)

Ejemplo de servicio systemd:
```ini
[Unit]
Description=Servicio API RAG
After=network.target

[Service]
Type=simple
User=tu-usuario
WorkingDirectory=/ruta/a/rag-api
ExecStart=/ruta/a/venv/bin/python main.py
Restart=always

[Install]
WantedBy=multi-user.target
```

## ğŸ”§ Variables de Entorno para ProducciÃ³n

Crea un archivo `.env` para configuraciÃ³n:

```env
# ChromaDB
CHROMA_HOST=localhost
CHROMA_PORT=8000
COLLECTION_NAME=pdf_documents_collection

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_EMBEDDING_MODEL=granite-embedding:278m
OLLAMA_GENERATION_MODEL=llama3.2:3b-instruct-q6_K

# Servidor
HOST=0.0.0.0
PORT=8080

# LÃ­mites
MAX_CHUNK_SIZE=1000
MAX_CONTEXT_LENGTH=2000
MAX_FILE_SIZE=50MB
```

## ğŸ¤ Contribuir

1. Hacer fork del repositorio
2. Crear una rama de caracterÃ­stica
3. Hacer tus cambios
4. Agregar pruebas si es aplicable
5. Enviar un pull request

## ğŸ“„ Licencia

Este proyecto estÃ¡ licenciado bajo la Licencia MIT - ver el archivo LICENSE para detalles.

## ğŸ™ Agradecimientos

- **Robyn**: Framework web Python de alto rendimiento
- **ChromaDB**: Base de datos vectorial de cÃ³digo abierto
- **LangChain**: Framework para desarrollar aplicaciones LLM
- **Ollama**: Plataforma de despliegue local de LLM
- **PyMuPDF**: Biblioteca de procesamiento de PDF

## ğŸ“ Soporte

Si encuentras problemas o tienes preguntas:

1. Revisa la secciÃ³n de soluciÃ³n de problemas arriba
2. Revisa los logs para mensajes de error
3. AsegÃºrate de que todos los requisitos previos estÃ©n instalados correctamente
4. Abre un issue en el repositorio

## ğŸ”¥ Casos de Uso Comunes

### AnÃ¡lisis de Documentos AcadÃ©micos
```bash
# Subir paper de investigaciÃ³n
curl -X POST -F "upload_file=@paper_investigacion.pdf" http://localhost:8080/upload-pdf

# Preguntas tÃ­picas
curl "http://localhost:8080/query?q=Â¿CuÃ¡l es la metodologÃ­a utilizada?"
curl "http://localhost:8080/query?q=Â¿CuÃ¡les son las conclusiones principales?"
```

### Procesamiento de Manuales TÃ©cnicos
```bash
# Subir manual
curl -X POST -F "upload_file=@manual_tecnico.pdf" http://localhost:8080/upload-pdf

# Consultas especÃ­ficas
curl "http://localhost:8080/query?q=Â¿CÃ³mo configurar el sistema?"
curl "http://localhost:8080/query?q=Â¿CuÃ¡les son los requisitos de instalaciÃ³n?"
```

### AnÃ¡lisis de Reportes Empresariales
```bash
# Subir reporte
curl -X POST -F "upload_file=@reporte_anual.pdf" http://localhost:8080/upload-pdf

# AnÃ¡lisis financiero
curl "http://localhost:8080/query?q=Â¿CuÃ¡les fueron los ingresos del Ãºltimo trimestre?"
curl "http://localhost:8080/query?q=Â¿QuÃ© estrategias se mencionan para el crecimiento?"
```

---

**Construido con â¤ï¸ usando tecnologÃ­as Python modernas para procesamiento rÃ¡pido e inteligente de documentos.**